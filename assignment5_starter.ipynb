{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/kelvinyeung/Documents/cs506/assign5\n",
      "Files in Current Directory: ['test.csv', 'assignment5_starter.ipynb', 'train.csv', '.ipynb_checkpoints', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "print(\"Files in Current Directory:\", os.listdir(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in test.csv: ['id', 'CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
      "Columns in train.csv: ['id', 'CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "# Print the column names\n",
    "print(\"Columns in test.csv:\", test_data.columns.tolist())\n",
    "print(\"Columns in train.csv:\", train_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        # Initialize the class with the number of neighbors and the distance metric\n",
    "        self.k = k\n",
    "        self.metric = distance_metric\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Store the training data\n",
    "        self.train_data = np.asarray(X_train)\n",
    "        self.train_labels = np.asarray(y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Initialize a list to store probability predictions\n",
    "        probabilities = []\n",
    "        \n",
    "        # Iterate over each test sample\n",
    "        for sample in X_test:\n",
    "            # Compute distances between the test sample and all training samples\n",
    "            distances = self._get_distances(sample, self.train_data)\n",
    "            \n",
    "            # Find the indices of the k nearest neighbors\n",
    "            nearest_indices = self._find_k_neighbors(distances)\n",
    "            \n",
    "            # Get the labels of the k nearest neighbors\n",
    "            neighbors_labels = self.train_labels[nearest_indices]\n",
    "            \n",
    "            # Calculate the proportion of class 1 among the neighbors (probability for class 1)\n",
    "            prob = np.mean(neighbors_labels == 1)\n",
    "            probabilities.append(prob)  # Store the probability\n",
    "        \n",
    "        # Return an array of probabilities for class 1\n",
    "        return np.array(probabilities)\n",
    "\n",
    "    def _get_distances(self, x_test, X_train):\n",
    "        # This helper function computes the distances between a test sample and all training samples\n",
    "        if self.metric == 'euclidean':\n",
    "            # Euclidean distance: square root of the sum of squared differences\n",
    "            return np.sqrt(np.sum((X_train - x_test) ** 2, axis=1))\n",
    "        elif self.metric == 'manhattan':\n",
    "            # Manhattan distance: sum of absolute differences\n",
    "            return np.sum(np.abs(X_train - x_test), axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Distance metric {self.metric} not supported.\")\n",
    "\n",
    "    def _find_k_neighbors(self, distances):\n",
    "        # Get the indices of the k smallest distances (k nearest neighbors)\n",
    "        return np.argsort(distances)[:self.k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_path, test_path):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    \n",
    "    # Target variable for the train data\n",
    "    target = train_data['Exited']\n",
    "    \n",
    "    # Drop unnecessary columns from train and test data\n",
    "    test_features = test_data.drop(columns=['id', 'CustomerId', 'Surname'])\n",
    "    train_features = train_data.drop(columns=['Exited', 'id', 'CustomerId', 'Surname'])\n",
    "    \n",
    "    # Handle missing values for numeric columns\n",
    "    numeric_cols = train_features.select_dtypes(include=[np.number]).columns\n",
    "    train_features[numeric_cols] = train_features[numeric_cols].fillna(train_features[numeric_cols].mean())\n",
    "    test_features[numeric_cols] = test_features[numeric_cols].fillna(test_features[numeric_cols].mean())\n",
    "    \n",
    "    # Concatenate train and test data for consistent preprocessing\n",
    "    combined_features = pd.concat([train_features, test_features], axis=0)\n",
    "    \n",
    "    # One-hot encode categorical variables such as 'Geography' and 'Gender'\n",
    "    combined_features = pd.get_dummies(combined_features, columns=['Geography', 'Gender'], drop_first=True)\n",
    "    \n",
    "    # Ensure no non-numeric columns remain\n",
    "    remaining_non_numeric = combined_features.select_dtypes(include=[object]).columns\n",
    "    assert remaining_non_numeric.empty, f\"Non-numeric columns detected: {remaining_non_numeric}\"\n",
    "    \n",
    "    # Standardize numeric columns using mean and standard deviation\n",
    "    numeric_columns = combined_features.select_dtypes(include=[np.number]).columns\n",
    "    column_means = combined_features[numeric_columns].mean()\n",
    "    column_stds = combined_features[numeric_columns].std()\n",
    "    combined_features[numeric_columns] = (combined_features[numeric_columns] - column_means) / column_stds\n",
    "    \n",
    "    # Ensure there are no missing values after preprocessing\n",
    "    assert not combined_features.isnull().any().any(), \"NaN values detected in the preprocessed data.\"\n",
    "    \n",
    "    # Split the processed data back into train and test sets\n",
    "    X_train_final = combined_features.iloc[:len(train_data), :]\n",
    "    X_test_final = combined_features.iloc[len(train_data):, :]\n",
    "    \n",
    "    # Return processed train data, target, and test data\n",
    "    return X_train_final.values, target.values, X_test_final.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation function\n",
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    # Convert inputs to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Shuffle the dataset before splitting into n_splits folds\n",
    "    shuffled_indices = np.arange(len(X))\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    X = X[shuffled_indices]\n",
    "    y = y[shuffled_indices]\n",
    "\n",
    "    # Calculate the size of each fold\n",
    "    fold_size = len(X) // n_splits\n",
    "\n",
    "    # Array to store the AUC scores for each fold\n",
    "    auc_scores = []\n",
    "\n",
    "    # Perform K-fold cross-validation\n",
    "    for fold in range(n_splits):\n",
    "        # Define the indices for the test set for the current fold\n",
    "        test_start = fold * fold_size\n",
    "        test_end = test_start + fold_size if fold != n_splits - 1 else len(X)\n",
    "\n",
    "        # Split the data into training and test sets\n",
    "        X_test_fold = X[test_start:test_end]\n",
    "        y_test_fold = y[test_start:test_end]\n",
    "        \n",
    "        X_train_fold = np.concatenate([X[:test_start], X[test_end:]], axis=0)\n",
    "        y_train_fold = np.concatenate([y[:test_start], y[test_end:]], axis=0)\n",
    "\n",
    "        # Train the KNN model on the training set\n",
    "        knn.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict probabilities or class scores for the test set\n",
    "        y_pred_fold = knn.predict(X_test_fold)\n",
    "\n",
    "        # Compute ROC AUC for the current fold\n",
    "        auc_value = compute_roc_auc(y_test_fold, y_pred_fold)\n",
    "        auc_scores.append(auc_value)\n",
    "\n",
    "    # Return all ROC AUC scores as a numpy array\n",
    "    return np.array(auc_scores)\n",
    "\n",
    "\n",
    "# Define ROC AUC calculation function\n",
    "def compute_roc_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the ROC AUC score manually using numpy.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true: numpy array of true labels (0 or 1)\n",
    "    y_pred: numpy array of predicted scores or labels (0 or 1)\n",
    "    \n",
    "    Returns:\n",
    "    roc_auc: float, the computed ROC AUC score\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Sort by predicted values (descending order)\n",
    "    sorted_indices = np.argsort(y_pred)[::-1]\n",
    "    sorted_true = y_true[sorted_indices]\n",
    "\n",
    "    # Calculate number of positive and negative labels\n",
    "    n_pos = np.sum(sorted_true == 1)\n",
    "    n_neg = np.sum(sorted_true == 0)\n",
    "\n",
    "    # Handle cases where there are no positive or no negative labels\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return 0.5  # Return 0.5 if no positive or negative examples\n",
    "\n",
    "    # Compute cumulative sums for TPR and FPR\n",
    "    tpr = np.cumsum(sorted_true) / n_pos  # True Positive Rate\n",
    "    fpr = np.cumsum(1 - sorted_true) / n_neg  # False Positive Rate\n",
    "\n",
    "    # Compute and return AUC using the trapezoidal rule (in one line)\n",
    "    return np.trapz(tpr, fpr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.8772697  0.88087318 0.88325544 0.87561336 0.87730473]\n",
      "k=3, metric=euclidean, Mean CV Score=0.8480572042323239\n",
      "k=3, metric=manhattan, Mean CV Score=0.8452072051893795\n",
      "k=5, metric=euclidean, Mean CV Score=0.8695421188551787\n",
      "k=5, metric=manhattan, Mean CV Score=0.8732697653540586\n",
      "k=7, metric=euclidean, Mean CV Score=0.8838304993897287\n",
      "k=7, metric=manhattan, Mean CV Score=0.8853425052057358\n",
      "k=9, metric=euclidean, Mean CV Score=0.8870391551160942\n",
      "k=9, metric=manhattan, Mean CV Score=0.8884834804801199\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('./train.csv', './test.csv')\n",
    "\n",
    "# Create and evaluate model\n",
    "knn = KNN(k=5, distance_metric='euclidean')\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_validate(X, y, knn)\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# TODO: hyperparamters tuning\n",
    "def tune_hyperparameters(X, y, k_values, distance_metrics):\n",
    "    best_score = 0\n",
    "    best_k = None\n",
    "    best_metric = None\n",
    "    \n",
    "    for k in k_values:\n",
    "        for metric in distance_metrics:\n",
    "            knn = KNN(k=k, distance_metric=metric)\n",
    "            cv_scores = cross_validate(X, y, knn)  # This returns an array of scores\n",
    "            mean_cv_score = np.mean(cv_scores)  # Compute the mean of the cross-validation scores\n",
    "            print(f\"k={k}, metric={metric}, Mean CV Score={mean_cv_score}\")\n",
    "            \n",
    "            # Compare the mean score with the current best score\n",
    "            if mean_cv_score > best_score:\n",
    "                best_score = mean_cv_score\n",
    "                best_k = k\n",
    "                best_metric = metric\n",
    "    \n",
    "    return best_k, best_metric\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter tuning (trying different values of k and metrics)\n",
    "k_values = [3, 5, 7, 9]  # You can expand this range as needed\n",
    "distance_metrics = ['euclidean', 'manhattan']  # Try different metrics\n",
    "\n",
    "best_k, best_metric = tune_hyperparameters(X, y, k_values, distance_metrics)\n",
    "\n",
    "# TODO: Train on full dataset with optimal hyperparameters and make predictions on test set\n",
    "knn = KNN(k=best_k, distance_metric=best_metric)\n",
    "knn.fit(X, y)\n",
    "test_predictions = knn.predict(X_test)\n",
    "\n",
    "# Save test predictions\n",
    "pd.DataFrame({'id': pd.read_csv('./test.csv')['id'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
